#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æ db/migrations ä¸‹çš„CSVæ–‡ä»¶
"""

import pandas as pd
import os
import sys
from pathlib import Path

def analyze_csv_file(file_path, sample_rows=10):
    """æ·±å…¥åˆ†æCSVæ–‡ä»¶"""
    print(f"\n{'='*80}")
    print(f"åˆ†ææ–‡ä»¶: {os.path.basename(file_path)}")
    print(f"{'='*80}")

    if not os.path.exists(file_path):
        print("æ–‡ä»¶ä¸å­˜åœ¨!")
        return None

    # å°è¯•ä¸åŒç¼–ç 
    encodings = ['gbk', 'gb2312', 'utf-8', 'utf-8-sig', 'latin1']
    df = None
    used_encoding = None

    for encoding in encodings:
        try:
            # è¯»å–å‰å‡ è¡Œæ£€æŸ¥
            df = pd.read_csv(file_path, encoding=encoding, nrows=100)
            print(f"[æˆåŠŸ] ä½¿ç”¨ç¼–ç : {encoding}")
            used_encoding = encoding
            break
        except Exception as e:
            continue

    if df is None:
        print("[å¤±è´¥] æ— æ³•è¯»å–æ–‡ä»¶ï¼Œå°è¯•è¿‡çš„ç¼–ç :", encodings)
        return None

    # åŸºæœ¬ä¿¡æ¯
    print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path)} å­—èŠ‚")
    print(f"è¡Œæ•°: {len(df)}")
    print(f"åˆ—æ•°: {len(df.columns)}")

    # åˆ—ä¿¡æ¯
    print("\n[åˆ—ä¿¡æ¯]:")
    for i, col in enumerate(df.columns):
        # æ£€æŸ¥åˆ—ç±»å‹å’Œç¤ºä¾‹å€¼
        non_null = df[col].notna().sum()
        null_count = df[col].isna().sum()
        sample_value = df[col].iloc[0] if non_null > 0 else "N/A"

        print(f"  {i+1:2d}. {col:30s} | éç©º: {non_null:4d} | ç©ºå€¼: {null_count:4d} | ç¤ºä¾‹: {str(sample_value)[:50]}")

    # æ•°æ®é¢„è§ˆ
    print(f"\n[é¢„è§ˆ] æ•°æ®é¢„è§ˆ (å‰{sample_rows}è¡Œ):")
    print(df.head(sample_rows).to_string())

    # æ•°æ®ç±»å‹ç»Ÿè®¡
    print(f"\n[ç±»å‹ç»Ÿè®¡] æ•°æ®ç±»å‹ç»Ÿè®¡:")
    print(df.dtypes.to_string())

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡è¡¨å¤´è¡Œ
    first_row_values = df.iloc[0].astype(str).values
    has_chinese_header = any(any('\u4e00' <= c <= '\u9fff' for c in str(val)) for val in first_row_values)

    if has_chinese_header:
        print("\n[è­¦å‘Š] ç¬¬ä¸€è¡Œæ•°æ®å¯èƒ½åŒ…å«ä¸­æ–‡è¡¨å¤´ï¼ˆéœ€è¦è·³è¿‡ç¬¬ä¸€è¡Œï¼‰")
        print("ç¬¬ä¸€è¡Œå†…å®¹:")
        print(df.iloc[0].to_dict())

    return df, used_encoding, has_chinese_header

def compare_with_passenger_record(df, filename):
    """æ¯”è¾ƒCSVç»“æ„ä¸ç°æœ‰PassengerRecordæ¨¡å‹"""
    print(f"\n[æ¨¡å‹æ¯”è¾ƒ] ä¸PassengerRecordæ¨¡å‹æ¯”è¾ƒ:")

    # PassengerRecordæ¨¡å‹å­—æ®µ
    passenger_record_fields = {
        'timestamp': 'DateTime',
        'station': 'str',
        'line': 'str',
        'direction': 'str',
        'passengers_in': 'int',
        'passengers_out': 'int',
        'metadata': 'JSON'
    }

    csv_columns = set(df.columns.str.lower().str.strip())
    model_fields = set(passenger_record_fields.keys())

    print(f"PassengerRecordå­—æ®µ: {model_fields}")
    print(f"CSVåˆ—å: {csv_columns}")

    # å¯»æ‰¾å¯èƒ½çš„æ˜ å°„
    potential_mappings = {}

    # å¸¸è§å­—æ®µæ˜ å°„
    field_mappings = {
        'timestamp': ['timestamp', 'æ—¶é—´', 'æ—¥æœŸ', 'datetime', 'time', 'date', 'yxrq', 'yxsj', 'ddsj', 'cfsj'],
        'station': ['station', 'ç«™ç‚¹', 'è½¦ç«™', 'zdmc', 'station_name'],
        'line': ['line', 'çº¿è·¯', 'line_code', 'xldm', 'yyxlbm'],
        'passengers_in': ['passengers_in', 'è¿›ç«™', 'ä¸Šå®¢', 'skl', 'in', 'ä¸Šè½¦'],
        'passengers_out': ['passengers_out', 'å‡ºç«™', 'ä¸‹å®¢', 'xkl', 'out', 'ä¸‹è½¦']
    }

    for model_field, possible_names in field_mappings.items():
        found = None
        for possible in possible_names:
            if possible in csv_columns:
                found = possible
                break
            # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
            for col in csv_columns:
                if possible.lower() in col.lower() or col.lower() in possible.lower():
                    found = col
                    break
            if found:
                break
        if found:
            potential_mappings[model_field] = found
            print(f"  âœ… {model_field} -> {found}")
        else:
            print(f"  âŒ {model_field}: æœªæ‰¾åˆ°åŒ¹é…åˆ—")

    return potential_mappings

def main():
    """ä¸»å‡½æ•°"""
    print("é“è·¯å®¢è¿æ•°æ®CSVæ–‡ä»¶åˆ†æ")
    print("="*80)

    csv_files = [
        "db/migrations/å®¢è¿ç«™ç‚¹ï¼ˆç«™ç‚¹åç§°ã€ç«™ç‚¹ç¼–å·ã€å¤‡æ³¨ï¼‰.csv",
        "db/migrations/åˆ—è½¦è¡¨ï¼ˆåˆ—è½¦ç¼–ç ã€åˆ—è½¦ä»£ç ã€åˆ—è½¦è¿é‡ï¼‰(2).csv",
        "db/migrations/è¿è¥çº¿è·¯å®¢è¿ç«™ï¼ˆè¿è¥çº¿è·¯ç¼–ç ã€ç«™ç‚¹idã€çº¿è·¯ç«™ç‚¹idã€ä¸Šä¸€ç«™idã€è¿è¥çº¿è·¯ç«™é—´è·ç¦» ã€ä¸‹ä¸€ç«™idã€è¿è¾“è·ç¦»ã€çº¿è·¯ä»£ç ï¼‰.csv",
        "db/migrations/é«˜é“å®¢è¿é‡ï¼ˆæˆéƒ½--é‡åº†ï¼‰ï¼ˆè¿è¥çº¿è·¯ç¼–ç ã€åˆ—è½¦ç¼–ç ã€ç«™ç‚¹idã€æ—¥æœŸã€åˆ°è¾¾æ—¶é—´ã€å‡ºå‘æ—¶é—´ã€ä¸Šå®¢é‡ã€ä¸‹å®¢é‡ç­‰ï¼Œèµ·ç‚¹ç«™ã€ç»ˆç‚¹ç«™ã€ç¥¨ä»·ã€æ”¶å…¥ç­‰ï¼‰.csv"
    ]

    all_analysis = {}

    for file_path in csv_files:
        if os.path.exists(file_path):
            result = analyze_csv_file(file_path)
            if result:
                df, encoding, has_chinese_header = result
                all_analysis[file_path] = {
                    'df': df,
                    'encoding': encoding,
                    'has_chinese_header': has_chinese_header,
                    'filename': os.path.basename(file_path)
                }

                # ç‰¹åˆ«åˆ†æé«˜é“å®¢æµæ•°æ®
                if "é«˜é“å®¢è¿é‡" in file_path:
                    print("\n[è¯¦ç»†åˆ†æ] é«˜é“å®¢æµæ•°æ®è¯¦ç»†åˆ†æ:")
                    mappings = compare_with_passenger_record(df, os.path.basename(file_path))

                    # æ£€æŸ¥æ—¶é—´ç›¸å…³å­—æ®µ
                    time_cols = [col for col in df.columns if any(word in col.lower() for word in ['æ—¶é—´', 'date', 'time', 'sj', 'rq'])]
                    print(f"æ—¶é—´ç›¸å…³åˆ—: {time_cols}")

                    # æ£€æŸ¥å®¢æµç›¸å…³å­—æ®µ
                    passenger_cols = [col for col in df.columns if any(word in col.lower() for word in ['å®¢', 'passenger', 'skl', 'xkl'])]
                    print(f"å®¢æµç›¸å…³åˆ—: {passenger_cols}")
        else:
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    # æ€»ç»“åˆ†æ
    print("\n" + "="*80)
    print("[åˆ†ææ€»ç»“] åˆ†ææ€»ç»“")
    print("="*80)

    for file_path, info in all_analysis.items():
        print(f"\n[æ–‡ä»¶] {info['filename']}:")
        print(f"  ç¼–ç : {info['encoding']}")
        print(f"  æ˜¯å¦æœ‰ä¸­æ–‡è¡¨å¤´: {info['has_chinese_header']}")
        print(f"  æ•°æ®å½¢çŠ¶: {info['df'].shape}")
        print(f"  åˆ—å: {list(info['df'].columns)}")

    # ç”Ÿæˆæ•°æ®å¯¼å…¥å»ºè®®
    print("\n" + "="*80)
    print("ğŸš€ æ•°æ®å¯¼å…¥å»ºè®®")
    print("="*80)

    # æ‰¾åˆ°ä¸»è¦çš„å®¢æµæ•°æ®æ–‡ä»¶
    passenger_file = None
    for file_path, info in all_analysis.items():
        if "é«˜é“å®¢è¿é‡" in info['filename']:
            passenger_file = info
            break

    if passenger_file:
        print("\nä¸»è¦å®¢æµæ•°æ®æ–‡ä»¶: é«˜é“å®¢è¿é‡.csv")
        df = passenger_file['df']

        # å»ºè®®çš„å­—æ®µæ˜ å°„
        print("\nå»ºè®®çš„å­—æ®µæ˜ å°„:")
        print("  1. æ—¶é—´å­—æ®µ: éœ€è¦ç¡®å®šå“ªä¸ªå­—æ®µä½œä¸ºtimestamp")
        print("  2. ç«™ç‚¹å­—æ®µ: éœ€è¦ä»ç«™ç‚¹IDæ˜ å°„åˆ°ç«™ç‚¹åç§°")
        print("  3. çº¿è·¯å­—æ®µ: éœ€è¦ä»è¿è¥çº¿è·¯ç¼–ç æ˜ å°„åˆ°çº¿è·¯åç§°")
        print("  4. å®¢æµå­—æ®µ: skl(ä¸Šå®¢é‡) -> passengers_in, xkl(ä¸‹å®¢é‡) -> passengers_out")

        # æ£€æŸ¥å¿…è¦çš„æ•°æ®å…³è”
        print("\néœ€è¦çš„æ•°æ®å…³è”:")
        print("  âœ“ ç«™ç‚¹IDåˆ°ç«™ç‚¹åç§°çš„æ˜ å°„ (ä»å®¢è¿ç«™ç‚¹.csv)")
        print("  âœ“ è¿è¥çº¿è·¯ç¼–ç åˆ°çº¿è·¯åç§°çš„æ˜ å°„ (ä»è¿è¥çº¿è·¯å®¢è¿ç«™.csv)")
        print("  âœ“ åˆ—è½¦ç¼–ç åˆ°åˆ—è½¦ä¿¡æ¯çš„æ˜ å°„ (ä»åˆ—è½¦è¡¨.csv)")

    return 0

if __name__ == "__main__":
    sys.exit(main())